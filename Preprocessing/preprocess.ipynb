{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c2f01d3-3849-49b4-8c81-de39ffa1bef3",
   "metadata": {},
   "source": [
    "Chapter 2: Working with Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5382da7f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ed9b394-e8ae-4c0f-8855-6fbb3a2649cd",
   "metadata": {},
   "source": [
    "2.2: Tokenizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e72f60-0d15-4213-ae8c-32f40c7b8444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\n",
    "        \"https://raw.githubusercontent.com/rasbt/\"\n",
    "        \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "        \"the-verdict.txt\"\n",
    "    )\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretreive(url,file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0689ec-e5f6-45bb-81a3-d5c1769cb7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_dataset = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5565779-6977-4eca-994c-9945ebcb1966",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09586417-31d5-420a-a8f6-e87770aa22c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec4c78c-2c97-4b9b-95de-b185fed1aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, My name is Yasharth.\"\n",
    "result = re.split(r'(\\s)',text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d7d3fe-c5c2-413b-98f2-17742ccec5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "result = re.findall(r'\\w+|[^\\w\\s]', raw_dataset)\n",
    "\n",
    "#print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a000f33-94a1-4229-80d6-1cf5031a1e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d61e30-c7a5-4247-9e68-9589fd1dee18",
   "metadata": {},
   "source": [
    "But we will be using following tokenizing for now, later shifting to tiktokenizeer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88c783b-030e-4886-adac-0afb9332684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_dataset)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b27fd-1bd4-434e-80af-82b51ddc1470",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bdba1d-e7a2-4c41-95a7-7874199b15c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21086524-efdf-49a4-b95f-90106169b505",
   "metadata": {},
   "source": [
    "2.3 Converting Tokens into Token ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba32784-417c-47ea-b7a6-be60d4382a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size= len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0647502f-5365-4f9f-a039-e35eaa8eadcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "#vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa1d1b0-dd75-4f6c-8af7-1b93cb555206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self,ids):\n",
    "               text=\" \".join([self.int_to_str[i] for i in ids])\n",
    "               #Replace spaces before the specified punctuations\n",
    "               text = re.split(r's\\+([,.?!\"()\\'])',r'\\1', text)\n",
    "               return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235572ea-dbe7-42c2-98ab-05e39674e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizer(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b3eab-9c51-41e6-97b2-abd6e148b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8abc22-ba24-435f-8514-59e677fad0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4704ce-3e03-4065-b9cb-2e4134099aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3b2a06-4d13-42bf-8234-fccac392836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84bbf53-dd49-4b23-97f2-b4f832fd986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b1fdb-e22e-443c-8a6b-e2b8aa3f7ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44e461b-ab6f-44ce-9865-bf7d18284de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7720d8c8-09b0-4fca-99b3-fc7ead84f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046b8da8-fdd7-4c2b-af3b-b38dccb72e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e7a590-b902-4a1f-8285-bda55cb35341",
   "metadata": {},
   "source": [
    "## 2.5: Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff386e-f2b8-410d-8399-95e5f79ef90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1d73a8-a891-43d9-9e85-5cba99aebade",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiktoken.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5259422-b39a-41f8-9f81-37f93d22daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba38e0c-dabb-4571-8fd0-676d147dd34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Hello Yasharth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee2a1e9-0aae-4c10-a19c-3dc6f310c1b5",
   "metadata": {},
   "source": [
    "## Data sampling with a sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e10e71-a21b-49d2-8f8b-376eb611262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daf74a2-6bde-4260-875f-3a1ea896bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample= enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f7dec0-6211-4864-afe2-5c93ed068c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151a6f8c-1945-4dd8-89e7-64d68d35bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(enc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a2eb6-6a33-47b0-b326-14dee6812dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c9fa02-d6fa-493a-a23f-89cfc277f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self,text,tokenizer,max_length,stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids =[]\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk= token_ids[i+1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e8b738-549d-4253-87d3-db9a94469f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. GPT-style Dataset (Listing 2.5)\n",
    "# --------------------------------------------------\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
    "\n",
    "            self.input_ids.append(torch.tensor(input_chunk, dtype=torch.long))\n",
    "            self.target_ids.append(torch.tensor(target_chunk, dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Example text\n",
    "# --------------------------------------------------\n",
    "text = \"hello world, this is a tiny gpt dataset\"\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Create dataset\n",
    "# --------------------------------------------------\n",
    "max_length = 8\n",
    "stride = 4\n",
    "\n",
    "dataset = GPTDatasetV1(\n",
    "    txt=text,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    "    stride=stride\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6. DataLoader\n",
    "# --------------------------------------------------\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 7. Inspect batches\n",
    "# --------------------------------------------------\n",
    "for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "    print(f\"\\nBatch {batch_idx}\")\n",
    "    print(\"Input IDs:\\n\", inputs)\n",
    "    print(\"Target IDs:\\n\", targets)\n",
    "\n",
    "    # decode first example in batch for clarity\n",
    "    print(\"Decoded input :\", tokenizer.decode(inputs[0].tolist()))\n",
    "    print(\"Decoded target:\", tokenizer.decode(targets[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cbe692-ec30-417d-a17d-b122d27beaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. GPT-style Dataset (Listing 2.5)\n",
    "# --------------------------------------------------\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # tokenize entire text once\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        # sliding window\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
    "\n",
    "            self.input_ids.append(\n",
    "                torch.tensor(input_chunk, dtype=torch.long)\n",
    "            )\n",
    "            self.target_ids.append(\n",
    "                torch.tensor(target_chunk, dtype=torch.long)\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. DataLoader factory (Listing 2.6)\n",
    "# --------------------------------------------------\n",
    "def create_dataloader_v1(\n",
    "    txt,\n",
    "    batch_size=4,\n",
    "    max_length=256,\n",
    "    stride=128,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "):\n",
    "    # A: initialize tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # B: create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # C + D: create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Example text (replace with your own file if needed)\n",
    "# --------------------------------------------------\n",
    "sample_text = (\n",
    "    \"This is a tiny example text used to demonstrate \"\n",
    "    \"how GPT-style dataloaders work with sliding windows.\"\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Create DataLoader with small context (demo)\n",
    "# --------------------------------------------------\n",
    "dataloader = create_dataloader_v1(\n",
    "    txt=sample_text,\n",
    "    batch_size=1,\n",
    "    max_length=4,\n",
    "    stride=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Inspect first two batches\n",
    "# --------------------------------------------------\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "first_batch = next(data_iter)\n",
    "second_batch = next(data_iter)\n",
    "\n",
    "print(\"First batch:\")\n",
    "print(first_batch)\n",
    "\n",
    "print(\"\\nSecond batch:\")\n",
    "print(second_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e4882-440e-41fb-b349-5c5afd13a6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. GPT-style Dataset (Listing 2.5)\n",
    "# --------------------------------------------------\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # tokenize entire text once\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        # sliding window\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
    "\n",
    "            self.input_ids.append(\n",
    "                torch.tensor(input_chunk, dtype=torch.long)\n",
    "            )\n",
    "            self.target_ids.append(\n",
    "                torch.tensor(target_chunk, dtype=torch.long)\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. DataLoader factory (Listing 2.6)\n",
    "# --------------------------------------------------\n",
    "def create_dataloader_v1(\n",
    "    txt,\n",
    "    batch_size=4,\n",
    "    max_length=8,\n",
    "    stride=2,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "):\n",
    "    # A: initialize tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # B: create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # C + D: create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Example text (replace with your own file if needed)\n",
    "# --------------------------------------------------\n",
    "sample_text = (\n",
    "    \"This is a tiny example text used to demonstrate \"\n",
    "    \"how GPT-style dataloaders work with sliding windows.\"\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Create DataLoader with small context (demo)\n",
    "# --------------------------------------------------\n",
    "dataloader = create_dataloader_v1(\n",
    "    txt=sample_text,\n",
    "    batch_size=1,\n",
    "    max_length=8,\n",
    "    stride=2,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Inspect first two batches\n",
    "# --------------------------------------------------\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "first_batch = next(data_iter)\n",
    "second_batch = next(data_iter)\n",
    "\n",
    "print(\"First batch:\")\n",
    "print(first_batch)\n",
    "\n",
    "print(\"\\nSecond batch:\")\n",
    "print(second_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbf7da3-7960-4c17-bcec-e9bdcb7b974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2,3,5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43553545-8249-4efb-ad6a-249c3c8f6991",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_size =6\n",
    "#output_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8757ea11-4a2e-425b-aea4-1da27927b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size,output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8b8adf-ece6-4c6f-af17-0df82f601308",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095587fb-d804-4a69-809e-9275f6b4bf47",
   "metadata": {},
   "source": [
    "  ## Data Sampling using sliding window (contd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915fc67-c234-47e2-9d10-8b136a21a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(text)\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            target_chunk = token_ids[i + 1 : i + max_length + 1]\n",
    "\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd3276b-ee35-45e6-8f35-ca5504c48ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt,batch_size=4,max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    dataset = GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
    "\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last = drop_last,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9816166d-2858-4bfe-9f02-325f7290b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\",\"r\", encoding = \"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0f6ce11e-f33a-4738-a463-c9f092284b33",
   "metadata": {},
   "source": [
    "print(raw_text[:1000])   # first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c3ac1e-ae59-40af-a0c4-2d1232bf38cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Characters read:\", len(raw_text))\n",
    "print(\"Preview:\", raw_text[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478f8e15-5904-4733-9888-64fbd57750a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_text[:1000])   # first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea162c-d7db-4114-9629-5c659a4fdb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1,max_length=8,stride=4,shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53407274-fc1a-4001-a7ba-07b371c2682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfc0c07-6a4a-4302-ae1c-742169a98e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,\n",
    "    batch_size=8,\n",
    "    max_length=8,\n",
    "    stride=4,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "input_ids, target_ids = next(data_iter)\n",
    "\n",
    "print(\"===== FIRST BATCH =====\")\n",
    "print(f\"Batch size     : {input_ids.shape[0]}\")\n",
    "print(f\"Sequence length: {input_ids.shape[1]}\")\n",
    "\n",
    "print(\"\\n--- Input IDs (x) ---\")\n",
    "print(input_ids)\n",
    "\n",
    "print(\"\\n--- Target IDs (y) ---\")\n",
    "print(target_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb578b-6def-4a36-980e-522ee215befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(\"\\n--- Decoded Input Sequences ---\")\n",
    "for i, seq in enumerate(input_ids):\n",
    "    print(f\"[Sample {i}] {tokenizer.decode(seq.tolist())}\")\n",
    "\n",
    "print(\"\\n--- Decoded Target Sequences ---\")\n",
    "for i, seq in enumerate(target_ids):\n",
    "    print(f\"[Sample {i}] {tokenizer.decode(seq.tolist())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1601f475-cae7-453a-a6b4-d8f4f0eb118c",
   "metadata": {},
   "source": [
    "## Creating Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e028ce-b0f4-403c-acd1-e3860a064fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([7,2,1,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdae45f-255b-425e-9e2d-93420d31e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size =6\n",
    "output_dim =3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd6243-aeb8-4208-bc1d-efc047520820",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfff5ba-22d7-4690-ad7c-e94cca1cd28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c7e855-2fa6-4143-8021-4bd1c16df1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer(torch.tensor([3]))\n",
    "#4th row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb793500-c48d-4181-8c52-64299746e1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer(torch.tensor([2])) \n",
    "#3rd row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8709c1b-af59-42e4-b63d-b203dd3f7c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max token ID in batch:\", input_ids.max().item())\n",
    "print(\"Embedding vocab size :\", embedding_layer.num_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c28c7b-840d-4c6b-a7e1-970e01f5c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(dataloader)\n",
    "input_ids, target_ids = next(data_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512db6e3-2afc-4b6a-9863-576730564d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FIX CHECK â†’\", input_ids.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a579676-5ba0-4b5d-bb58-100987f9ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3a9642-250c-48af-a920-275b6f1cd28b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d632d-9fd2-445e-9015-00c16e3c151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed3d72-9504-4c82-89cb-97ac1056df46",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472b163c-729d-4ed8-a300-031e96d220bc",
   "metadata": {},
   "source": [
    "## Encoding word position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a8441d-2733-4576-b3e5-9cb0c25f762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50527\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa4a9b-67fc-4b95-914b-6b33f5cfae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length =4\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8,max_length=max_length,stride=max_length,shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs,targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8efbf-6df8-4123-b21b-8b01201780c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Token ID's: \\n \",inputs)\n",
    "\n",
    "print(\"shape: \\n\",inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3b1ee6-97ac-4ad9-bed2-d1842539861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding=token_embedding_layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d048b-ca7c-4697-9864-26e9037ae022",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b8a736-83bb-47a0-b476-acdb70b80013",
   "metadata": {},
   "source": [
    "meaning 8 rows 4 coloumns, and 256 dimension vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73e4b1a-4b92-4182-8d71-1be74b0eb5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#token_embedding[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c27caf8-7f75-4451-badd-668f041fbb44",
   "metadata": {},
   "source": [
    "### Adding positional information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2848e6-6ce8-47ab-961a-ce0a4bd772c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1108edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(max_length) # 0 1 2 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c075b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embedding = pos_embedding_layer(torch.arange(max_length)) #it will give embeddings for positions 0,1,2,3\n",
    "print(pos_embedding.shape) #(4, 256)\n",
    "print(pos_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91862ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embedding_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afb25b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding[0] +pos_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825caad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embedding = token_embedding + pos_embedding\n",
    "input_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c748f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding[0] +pos_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1adc768",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding + pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c31971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c4e598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs-from-scratch-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
